{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_logret1\"] = log_ret_1\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom5\"] = px.pct_change(5)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom20\"] = px.pct_change(20)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom60\"] = px.pct_change(60)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol20\"] = ret_1.rolling(20).std()\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol60\"] = ret_1.rolling(60).std()\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_ma_ratio20\"] = px / px.rolling(20).mean() - 1.0\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_ma_ratio60\"] = px / px.rolling(60).mean() - 1.0\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_logvol\"] = log_vol\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol_z20\"] = (\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol_chg5\"] = log_vol.diff(5)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_ret1\"] = ret_1\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_logret1\"] = log_ret_1\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom5\"] = px.pct_change(5)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom20\"] = px.pct_change(20)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_mom60\"] = px.pct_change(60)\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol20\"] = ret_1.rolling(20).std()\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol60\"] = ret_1.rolling(60).std()\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_ma_ratio20\"] = px / px.rolling(20).mean() - 1.0\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_ma_ratio60\"] = px / px.rolling(60).mean() - 1.0\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_logvol\"] = log_vol\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol_z20\"] = (\n",
      "/var/folders/zx/7nt78dzs2lbccbwfzr2blrrr0000gn/T/ipykernel_32751/1071302152.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{inst}_vol_chg5\"] = log_vol.diff(5)\n"
     ]
    }
   ],
   "source": [
    "DATE = \"2025-02-28\"\n",
    "prev_date = \"2024-02-28\"\n",
    "\n",
    "df_prices = pd.read_csv(f\"data/{DATE}/prices.csv\", parse_dates=[\"date\"])\n",
    "df_cash_rates = pd.read_csv(f\"data/{DATE}/cash_rate.csv\", parse_dates=[\"date\"])\n",
    "df_signals = pd.read_csv(f\"data/{DATE}/signals.csv\", parse_dates=[\"date\"])\n",
    "df_volumes = pd.read_csv(f\"data/{DATE}/volumes.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "dfs = [df_prices, df_cash_rates, df_signals, df_volumes]\n",
    "\n",
    "df = (\n",
    "    reduce(\n",
    "        lambda left, right: pd.merge(left, right, on=\"date\", how=\"left\"),\n",
    "        dfs,\n",
    "    )\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "def add_periodic_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    date = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    day_of_week = date.dt.dayofweek\n",
    "    day_of_year = date.dt.dayofyear\n",
    "    month = date.dt.month\n",
    "    week_of_year = date.dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    df[\"doy_sin\"] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "    df[\"doy_cos\"] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * month / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * month / 12)\n",
    "    df[\"woy_sin\"] = np.sin(2 * np.pi * week_of_year / 52.18)\n",
    "    df[\"woy_cos\"] = np.cos(2 * np.pi * week_of_year / 52.18)\n",
    "    return df\n",
    "\n",
    "def add_instrument_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    for inst in [f\"INSTRUMENT_{i}\" for i in range(1, 11)]:\n",
    "        px = df[inst].astype(float)\n",
    "        ret_1 = px.pct_change()\n",
    "        log_ret_1 = np.log(px).diff()\n",
    "\n",
    "        df[f\"{inst}_ret1\"] = ret_1\n",
    "        df[f\"{inst}_logret1\"] = log_ret_1\n",
    "        df[f\"{inst}_mom5\"] = px.pct_change(5)\n",
    "        df[f\"{inst}_mom20\"] = px.pct_change(20)\n",
    "        df[f\"{inst}_mom60\"] = px.pct_change(60)\n",
    "        df[f\"{inst}_vol20\"] = ret_1.rolling(20).std()\n",
    "        df[f\"{inst}_vol60\"] = ret_1.rolling(60).std()\n",
    "        df[f\"{inst}_ma_ratio20\"] = px / px.rolling(20).mean() - 1.0\n",
    "        df[f\"{inst}_ma_ratio60\"] = px / px.rolling(60).mean() - 1.0\n",
    "\n",
    "        vol_col = f\"{inst}_vol\"\n",
    "        if vol_col in df.columns:\n",
    "            log_vol = np.log1p(df[vol_col].astype(float))\n",
    "            df[f\"{inst}_logvol\"] = log_vol\n",
    "            df[f\"{inst}_vol_z20\"] = (\n",
    "                (log_vol - log_vol.rolling(20).mean())\n",
    "                / (log_vol.rolling(20).std() + 1e-12)\n",
    "            )\n",
    "            df[f\"{inst}_vol_chg5\"] = log_vol.diff(5)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_periodic_date_features(df)\n",
    "df = add_instrument_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "data-info",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2910 entries, 0 to 2909\n",
      "Columns: 203 entries, date to INSTRUMENT_10_vol_chg5\n",
      "dtypes: datetime64[ns](1), float64(192), int64(10)\n",
      "memory usage: 4.5 MB\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUMENTS = [f\"INSTRUMENT_{i}\" for i in range(1, 11)]\n",
    "DATE_FEATURES = [\n",
    "    \"dow_sin\", \"dow_cos\",\n",
    "    \"doy_sin\", \"doy_cos\",\n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"woy_sin\", \"woy_cos\",\n",
    "]\n",
    "MACRO_FEATURES = [\n",
    "    c for c in [\n",
    "        \"1mo\", \"1.5month\", \"2mo\", \"3mo\", \"4mo\", \"6mo\",\n",
    "        \"1yr\", \"2yr\", \"3yr\", \"5yr\", \"7yr\", \"10yr\", \"20yr\", \"30yr\",\n",
    "    ]\n",
    "    if c in df.columns and df[c].notna().mean() > 0.5\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray):\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return e / (e.sum() + 1e-12)\n",
    "\n",
    "def cap_and_renorm(w: pd.Series, cap=0.25) -> pd.Series:\n",
    "    w = w.clip(lower=0.0, upper=cap)\n",
    "    s = float(w.sum())\n",
    "    if s <= 0:\n",
    "        return pd.Series(1.0 / len(w), index=w.index)\n",
    "    return w / s\n",
    "\n",
    "def build_feature_cols(df: pd.DataFrame, inst: str):\n",
    "    own_cols = [\n",
    "        col for col in df.columns\n",
    "        if col == inst or col.startswith(f\"{inst}_\")\n",
    "    ]\n",
    "    cols = DATE_FEATURES + MACRO_FEATURES + own_cols\n",
    "    return [col for col in cols if col in df.columns]\n",
    "\n",
    "def make_labels_next_return(df: pd.DataFrame, inst: str):\n",
    "    x = df[inst].astype(float)\n",
    "    return x.shift(-1) / x - 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "train-predict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results by instrument:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instrument</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_val</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INSTRUMENT_7</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.8863</td>\n",
       "      <td>0.7540</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INSTRUMENT_10</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.8362</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INSTRUMENT_1</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INSTRUMENT_2</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>0.6587</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INSTRUMENT_8</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INSTRUMENT_9</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.7063</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INSTRUMENT_5</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.5982</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INSTRUMENT_3</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.5563</td>\n",
       "      <td>0.6706</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INSTRUMENT_4</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INSTRUMENT_6</td>\n",
       "      <td>38</td>\n",
       "      <td>2657</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.4757</td>\n",
       "      <td>0.5992</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      instrument  n_features  n_train  n_val  val_rmse  val_corr  \\\n",
       "6   INSTRUMENT_7          38     2657    252    0.0047    0.8863   \n",
       "9  INSTRUMENT_10          38     2657    252    0.0236    0.8362   \n",
       "0   INSTRUMENT_1          38     2657    252    0.0054    0.7842   \n",
       "1   INSTRUMENT_2          38     2657    252    0.0081    0.7803   \n",
       "7   INSTRUMENT_8          38     2657    252    0.0130    0.6564   \n",
       "8   INSTRUMENT_9          38     2657    252    0.0224    0.6540   \n",
       "4   INSTRUMENT_5          38     2657    252    0.0065    0.5982   \n",
       "2   INSTRUMENT_3          38     2657    252    0.0065    0.5563   \n",
       "3   INSTRUMENT_4          38     2657    252    0.0080    0.5229   \n",
       "5   INSTRUMENT_6          38     2657    252    0.0032    0.4757   \n",
       "\n",
       "   val_hit_rate                                        best_params  \n",
       "6        0.7540  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "9        0.7778  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "0        0.7500  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "1        0.6587  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "7        0.6667  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "8        0.7063  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "4        0.6151  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "2        0.6706  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "3        0.6667  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  \n",
       "5        0.5992  {'colsample_bytree': 1.0, 'learning_rate': 0.0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0101     0.675        0.6865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300, 'reg_lambda': 3.0, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def train_predict_scores(\n",
    "    df: pd.DataFrame,\n",
    "    min_train: int = 750,\n",
    "    val_size: int = 252,\n",
    "    verbose = True\n",
    "):\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [150, 300],\n",
    "        \"max_depth\": [4, 5],\n",
    "        \"learning_rate\": [0.03, 0.05],\n",
    "        \"subsample\": [1.0], #[0.8, 1.0],\n",
    "        \"colsample_bytree\": [1.0], #[0.8, 1.0],\n",
    "        \"reg_lambda\": [1.0, 3.0, 5.0],\n",
    "    }\n",
    "\n",
    "    preds = {}\n",
    "    validation_rows = []\n",
    "    fitted_models = {}\n",
    "\n",
    "    for inst in INSTRUMENTS:\n",
    "        feat_cols = build_feature_cols(df, inst)\n",
    "        X_all = df.iloc[:-1][feat_cols].copy()\n",
    "        y_all = make_labels_next_return(df, inst).iloc[:-1]\n",
    "\n",
    "        mask = ~y_all.isna()\n",
    "        X = X_all.loc[mask]\n",
    "        y = y_all.loc[mask]\n",
    "\n",
    "        if len(X) < min_train + val_size:\n",
    "            preds[inst] = 0.0\n",
    "            validation_rows.append(\n",
    "                {\n",
    "                    \"instrument\": inst,\n",
    "                    \"n_features\": len(feat_cols),\n",
    "                    \"n_train\": 0,\n",
    "                    \"n_val\": 0,\n",
    "                    \"best_params\": None,\n",
    "                    \"val_rmse\": np.nan,\n",
    "                    \"val_corr\": np.nan,\n",
    "                    \"val_hit_rate\": np.nan,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        X_train = X.iloc[:-val_size]\n",
    "        y_train = y.iloc[:-val_size]\n",
    "        X_val = X.iloc[-val_size:]\n",
    "        y_val = y.iloc[-val_size:]\n",
    "\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            model = XGBRegressor(\n",
    "                objective=\"reg:squarederror\",\n",
    "                tree_method=\"hist\",\n",
    "                random_state=0,\n",
    "                n_jobs=4,\n",
    "                **params,\n",
    "            )\n",
    "            model.fit(X_train.values, y_train.values)\n",
    "\n",
    "            val_pred = model.predict(X_val.values)\n",
    "            val_corr = pd.Series(val_pred).corr(pd.Series(y_val.values))\n",
    "            val_corr = 0.0 if pd.isna(val_corr) else float(val_corr)\n",
    "\n",
    "            if val_corr > best_score:\n",
    "                best_score = val_corr\n",
    "                best_params = params\n",
    "\n",
    "        final_model = XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=0,\n",
    "            n_jobs=1,\n",
    "            **best_params,\n",
    "        )\n",
    "        final_model.fit(X.values, y.values)\n",
    "\n",
    "        val_pred = final_model.predict(X_val.values)\n",
    "        val_rmse = float(np.sqrt(np.mean((val_pred - y_val.values) ** 2)))\n",
    "        val_corr = pd.Series(val_pred).corr(pd.Series(y_val.values))\n",
    "        val_corr = 0.0 if pd.isna(val_corr) else float(val_corr)\n",
    "        val_hit_rate = float(((val_pred > 0) == (y_val.values > 0)).mean())\n",
    "\n",
    "        validation_rows.append(\n",
    "            {\n",
    "                \"instrument\": inst,\n",
    "                \"n_features\": len(feat_cols),\n",
    "                \"n_train\": len(X_train),\n",
    "                \"n_val\": len(X_val),\n",
    "                \"best_params\": best_params,\n",
    "                \"val_rmse\": val_rmse,\n",
    "                \"val_corr\": val_corr,\n",
    "                \"val_hit_rate\": val_hit_rate,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        x_last = df.loc[df.index[-1], feat_cols].astype(float)\n",
    "        preds[inst] = float(final_model.predict([x_last.values])[0])\n",
    "        fitted_models[inst] = final_model\n",
    "\n",
    "    validation_df = pd.DataFrame(validation_rows).sort_values(\"val_corr\", ascending=False)\n",
    "    confidence_series = (\n",
    "        validation_df.set_index(\"instrument\")[\"val_corr\"]\n",
    "        .reindex(INSTRUMENTS)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Validation results by instrument:\")\n",
    "        display(\n",
    "            validation_df[\n",
    "                [\n",
    "                    \"instrument\",\n",
    "                    \"n_features\",\n",
    "                    \"n_train\",\n",
    "                    \"n_val\",\n",
    "                    \"val_rmse\",\n",
    "                    \"val_corr\",\n",
    "                    \"val_hit_rate\",\n",
    "                    \"best_params\",\n",
    "                ]\n",
    "            ].round(4)\n",
    "        )\n",
    "\n",
    "    print(\"\\nAverage validation metrics:\")\n",
    "    display(\n",
    "        validation_df[[\"val_rmse\", \"val_corr\", \"val_hit_rate\"]]\n",
    "        .mean()\n",
    "        .to_frame(\"mean\")\n",
    "        .T.round(4)\n",
    "    )\n",
    "\n",
    "    return fitted_models, pd.Series(preds), validation_df, best_params, confidence_series\n",
    "\n",
    "models, pred, validation_df, best_params, confidence = train_predict_scores(df)\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f55e341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrument\n",
      "INSTRUMENT_1     0.784219\n",
      "INSTRUMENT_2     0.780272\n",
      "INSTRUMENT_3     0.556320\n",
      "INSTRUMENT_4     0.522885\n",
      "INSTRUMENT_5     0.598179\n",
      "INSTRUMENT_6     0.475689\n",
      "INSTRUMENT_7     0.886279\n",
      "INSTRUMENT_8     0.656354\n",
      "INSTRUMENT_9     0.654004\n",
      "INSTRUMENT_10    0.836236\n",
      "Name: val_corr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "weights",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: HiddenLabel_round_2.csv\n",
      "INSTRUMENT_1     0.099683\n",
      "INSTRUMENT_2     0.099412\n",
      "INSTRUMENT_3     0.099464\n",
      "INSTRUMENT_4     0.099526\n",
      "INSTRUMENT_5     0.099746\n",
      "INSTRUMENT_6     0.099823\n",
      "INSTRUMENT_7     0.099038\n",
      "INSTRUMENT_8     0.099904\n",
      "INSTRUMENT_9     0.099974\n",
      "INSTRUMENT_10    0.103430\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def construct_weights(\n",
    "    pred: pd.Series,\n",
    "    confidence: float | pd.Series,\n",
    "    beta=3.0,\n",
    "    cap=0.25,\n",
    "    smooth_alpha=1.0,\n",
    "    prev_weights: pd.Series | None = None,\n",
    "):\n",
    "    scores = pred.copy()\n",
    "\n",
    "    # allow scalar or per-asset confidence\n",
    "    if isinstance(confidence, pd.Series):\n",
    "        conf = confidence.reindex(INSTRUMENTS).fillna(0.0)\n",
    "        logits = beta * conf * scores\n",
    "    else:\n",
    "        logits = beta * confidence * scores\n",
    "\n",
    "    w = pd.Series(softmax(logits.values), index=INSTRUMENTS)\n",
    "    w = cap_and_renorm(w, cap=cap)\n",
    "\n",
    "    if prev_weights is not None:\n",
    "        prev_weights = prev_weights.reindex(INSTRUMENTS).fillna(0.0)\n",
    "        prev_weights = prev_weights / prev_weights.sum()\n",
    "        w = smooth_alpha * w + (1.0 - smooth_alpha) * prev_weights\n",
    "        w = cap_and_renorm(w, cap=cap)\n",
    "\n",
    "    return w\n",
    "\n",
    "def write_submission(weights: pd.Series, team_name: str, round_n: int, out_path=\".\"):\n",
    "    out = pd.DataFrame({\"asset\": weights.index, \"weight\": weights.values})\n",
    "    fname = f\"{team_name}_round_{round_n}.csv\"\n",
    "    out.to_csv(f\"{out_path}/{fname}\", index=False)\n",
    "    return fname\n",
    "\n",
    "w = construct_weights(pred, confidence)\n",
    "fname = write_submission(w, team_name=\"HiddenLabel\", round_n=2, out_path=\".\")\n",
    "print(\"Wrote:\", fname)\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "backtest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.6929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0106    0.7399        0.6929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0102     0.739        0.7079"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.7446</td>\n",
       "      <td>0.7278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0102    0.7446        0.7278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.7684</td>\n",
       "      <td>0.7369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0114    0.7684        0.7369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_corr</th>\n",
       "      <th>val_hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.8038</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      val_rmse  val_corr  val_hit_rate\n",
       "mean    0.0134    0.8038         0.754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 204\u001b[0m\n\u001b[1;32m    146\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    147\u001b[0m         {\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_test_points\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(bt_df), \u001b[38;5;28mlen\u001b[39m(bt_df), \u001b[38;5;28mlen\u001b[39m(bt_df), \u001b[38;5;28mlen\u001b[39m(bt_df)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy_net\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy_gross\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop3_predicted\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: pred_df,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactuals\u001b[39m\u001b[38;5;124m\"\u001b[39m: actual_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics,\n\u001b[1;32m    201\u001b[0m     }\n\u001b[0;32m--> 204\u001b[0m bt \u001b[38;5;241m=\u001b[39m backtest_model_fast(\n\u001b[1;32m    205\u001b[0m     df,\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    208\u001b[0m display(bt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    210\u001b[0m ax \u001b[38;5;241m=\u001b[39m bt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequity_curve\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m), title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast Out-of-Sample Backtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy_net\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[98], line 65\u001b[0m, in \u001b[0;36mbacktest_model_fast\u001b[0;34m(df, start_idx, retrain_every, holding_period, beta, cap, filter_bad_instruments, transaction_cost_bps)\u001b[0m\n\u001b[1;32m     62\u001b[0m prev_weights \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;241m0.0\u001b[39m, index\u001b[38;5;241m=\u001b[39mINSTRUMENTS)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_idx, \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m-\u001b[39m holding_period, retrain_every):\n\u001b[0;32m---> 65\u001b[0m     fitted_models, pred, validation_df, best_params, _ \u001b[38;5;241m=\u001b[39m train_predict_scores(df\u001b[38;5;241m.\u001b[39miloc[: t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(), verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m     selected \u001b[38;5;241m=\u001b[39m validation_df\u001b[38;5;241m.\u001b[39mloc[validation_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_corr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstrument\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filter_bad_instruments:\n",
      "Cell \u001b[0;32mIn[93], line 63\u001b[0m, in \u001b[0;36mtrain_predict_scores\u001b[0;34m(df, min_train, val_size, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m ParameterGrid(param_grid):\n\u001b[1;32m     56\u001b[0m     model \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[1;32m     57\u001b[0m         objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m         tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mvalues, y_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     65\u001b[0m     val_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     66\u001b[0m     val_corr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(val_pred)\u001b[38;5;241m.\u001b[39mcorr(pd\u001b[38;5;241m.\u001b[39mSeries(y_val\u001b[38;5;241m.\u001b[39mvalues))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/xgboost/sklearn.py:1365\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1363\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1366\u001b[0m     params,\n\u001b[1;32m   1367\u001b[0m     train_dmatrix,\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1369\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1370\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1371\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1372\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1373\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1374\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1375\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1376\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1377\u001b[0m )\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/xgboost/core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/xgboost/training.py:199\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/xgboost/core.py:2434\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2433\u001b[0m     _check_call(\n\u001b[0;32m-> 2434\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2435\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2436\u001b[0m         )\n\u001b[1;32m   2437\u001b[0m     )\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2439\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_forward_returns(df: pd.DataFrame, horizon: int) -> pd.DataFrame:\n",
    "    out = {}\n",
    "    for inst in INSTRUMENTS:\n",
    "        px = df[inst].astype(float)\n",
    "        out[inst] = px.shift(-horizon) / px - 1.0\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def trailing_realized_vol(df: pd.DataFrame, end_idx: int, window: int = 20) -> pd.Series:\n",
    "    vols = {}\n",
    "    for inst in INSTRUMENTS:\n",
    "        ret1 = df[inst].astype(float).pct_change()\n",
    "        vols[inst] = ret1.iloc[:end_idx].tail(window).std()\n",
    "    vols = pd.Series(vols).replace([np.inf, -np.inf], np.nan)\n",
    "    return vols.fillna(vols.median()).clip(lower=1e-4)\n",
    "\n",
    "\n",
    "def prediction_to_weights(\n",
    "    pred_row: pd.Series,\n",
    "    vol_row: pd.Series,\n",
    "    beta: float = 3.0,\n",
    "    cap: float = 0.25,\n",
    "    selected: list[str] | None = None,\n",
    "):\n",
    "    scores = pred_row.copy()\n",
    "\n",
    "    if selected is not None:\n",
    "        excluded = [inst for inst in INSTRUMENTS if inst not in selected]\n",
    "        scores.loc[excluded] = np.nan\n",
    "\n",
    "    valid = scores.notna()\n",
    "    if valid.sum() == 0:\n",
    "        return pd.Series(1.0 / len(INSTRUMENTS), index=INSTRUMENTS)\n",
    "\n",
    "    z = scores.loc[valid]\n",
    "    z = (z - z.mean()) / (z.std(ddof=0) + 1e-12)\n",
    "    z = z / vol_row.loc[valid]\n",
    "\n",
    "    raw = pd.Series(0.0, index=INSTRUMENTS)\n",
    "    scaled = beta * z\n",
    "    scaled = scaled - scaled.max()\n",
    "    raw.loc[valid] = np.exp(scaled)\n",
    "\n",
    "    return cap_and_renorm(raw, cap=cap)\n",
    "\n",
    "\n",
    "def backtest_model_fast(\n",
    "    df: pd.DataFrame,\n",
    "    start_idx: int = 1500,\n",
    "    retrain_every: int = 120,\n",
    "    holding_period: int = 120,\n",
    "    beta: float = 3.0,\n",
    "    cap: float = 0.25,\n",
    "    filter_bad_instruments: bool = True,\n",
    "    transaction_cost_bps: float = 0,\n",
    "):\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    forward_returns = make_forward_returns(df, horizon=holding_period)\n",
    "\n",
    "    pred_rows = []\n",
    "    bt_rows = []\n",
    "    prev_weights = pd.Series(0.0, index=INSTRUMENTS)\n",
    "\n",
    "    for t in range(start_idx, len(df) - holding_period, retrain_every):\n",
    "        fitted_models, pred, validation_df, best_params, _ = train_predict_scores(df.iloc[: t + 1].copy(), verbose = False)\n",
    "\n",
    "        selected = validation_df.loc[validation_df[\"val_corr\"] > 0, \"instrument\"].tolist()\n",
    "        if not filter_bad_instruments:\n",
    "            selected = INSTRUMENTS.copy()\n",
    "\n",
    "        vol_row = trailing_realized_vol(df, end_idx=t, window=20)\n",
    "        weights = prediction_to_weights(\n",
    "            pred,\n",
    "            vol_row,\n",
    "            beta=beta,\n",
    "            cap=cap,\n",
    "            selected=selected,\n",
    "        )\n",
    "\n",
    "        realized = forward_returns.loc[t, INSTRUMENTS].astype(float)\n",
    "        gross_return = float((weights * realized).sum())\n",
    "\n",
    "        turnover = float(np.abs(weights - prev_weights).sum())\n",
    "        tc = turnover * transaction_cost_bps / 10000.0\n",
    "        net_return = gross_return - tc\n",
    "\n",
    "        eq_weights = pd.Series(1.0 / len(INSTRUMENTS), index=INSTRUMENTS)\n",
    "        eq_return = float((eq_weights * realized).sum())\n",
    "\n",
    "        top3 = pred.rank(ascending=False, method=\"first\") <= 3\n",
    "        if filter_bad_instruments and len(selected) > 0:\n",
    "            top3 = top3 & top3.index.to_series().isin(selected).values\n",
    "        top3_weights = top3.astype(float)\n",
    "        if top3_weights.sum() > 0:\n",
    "            top3_weights = top3_weights / top3_weights.sum()\n",
    "        else:\n",
    "            top3_weights = eq_weights.copy()\n",
    "        top3_return = float((top3_weights * realized).sum())\n",
    "\n",
    "        pred_rows.append({\"date\": df.loc[t, \"date\"], **pred.to_dict()})\n",
    "        bt_rows.append(\n",
    "            {\n",
    "                \"date\": df.loc[t, \"date\"],\n",
    "                \"selected_count\": len(selected),\n",
    "                \"gross_return\": gross_return,\n",
    "                \"net_return\": net_return,\n",
    "                \"equal_weight_return\": eq_return,\n",
    "                \"top3_return\": top3_return,\n",
    "                \"turnover\": turnover,\n",
    "                \"transaction_cost\": tc,\n",
    "                **{f\"w_{inst}\": weights[inst] for inst in INSTRUMENTS},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        prev_weights = weights.copy()\n",
    "\n",
    "    pred_df = pd.DataFrame(pred_rows)\n",
    "    bt_df = pd.DataFrame(bt_rows)\n",
    "\n",
    "    actual_df = forward_returns.iloc[start_idx : len(df) - holding_period : retrain_every].reset_index(drop=True)\n",
    "    actual_df.insert(\n",
    "        0,\n",
    "        \"date\",\n",
    "        df[\"date\"].iloc[start_idx : len(df) - holding_period : retrain_every].reset_index(drop=True),\n",
    "    )\n",
    "\n",
    "    pred_mat = pred_df[INSTRUMENTS]\n",
    "    act_mat = actual_df[INSTRUMENTS]\n",
    "\n",
    "    ic_by_date = pred_mat.corrwith(act_mat, axis=1, method=\"spearman\")\n",
    "    hit_rate = ((pred_mat.values > 0) == (act_mat.values > 0)).mean()\n",
    "\n",
    "    strategy_curve = (1 + bt_df[\"net_return\"]).cumprod()\n",
    "    gross_curve = (1 + bt_df[\"gross_return\"]).cumprod()\n",
    "    equal_weight_curve = (1 + bt_df[\"equal_weight_return\"]).cumprod()\n",
    "    top3_curve = (1 + bt_df[\"top3_return\"]).cumprod()\n",
    "\n",
    "    ann_factor = np.sqrt(252 / holding_period)\n",
    "\n",
    "    def sharpe(x: pd.Series) -> float:\n",
    "        return float(x.mean() / (x.std(ddof=1) + 1e-12) * ann_factor)\n",
    "\n",
    "    def max_dd(curve: pd.Series) -> float:\n",
    "        return float((curve / curve.cummax() - 1.0).min())\n",
    "\n",
    "    metrics = pd.DataFrame(\n",
    "        {\n",
    "            \"n_test_points\": [len(bt_df), len(bt_df), len(bt_df), len(bt_df)],\n",
    "            \"mean_spearman_ic\": [ic_by_date.mean(), np.nan, np.nan, np.nan],\n",
    "            \"hit_rate\": [hit_rate, np.nan, np.nan, np.nan],\n",
    "            \"annualized_sharpe\": [\n",
    "                sharpe(bt_df[\"net_return\"]),\n",
    "                sharpe(bt_df[\"gross_return\"]),\n",
    "                sharpe(bt_df[\"equal_weight_return\"]),\n",
    "                sharpe(bt_df[\"top3_return\"]),\n",
    "            ],\n",
    "            \"avg_period_return\": [\n",
    "                bt_df[\"net_return\"].mean(),\n",
    "                bt_df[\"gross_return\"].mean(),\n",
    "                bt_df[\"equal_weight_return\"].mean(),\n",
    "                bt_df[\"top3_return\"].mean(),\n",
    "            ],\n",
    "            \"vol_period_return\": [\n",
    "                bt_df[\"net_return\"].std(ddof=1),\n",
    "                bt_df[\"gross_return\"].std(ddof=1),\n",
    "                bt_df[\"equal_weight_return\"].std(ddof=1),\n",
    "                bt_df[\"top3_return\"].std(ddof=1),\n",
    "            ],\n",
    "            \"total_return\": [\n",
    "                strategy_curve.iloc[-1] - 1.0,\n",
    "                gross_curve.iloc[-1] - 1.0,\n",
    "                equal_weight_curve.iloc[-1] - 1.0,\n",
    "                top3_curve.iloc[-1] - 1.0,\n",
    "            ],\n",
    "            \"max_drawdown\": [\n",
    "                max_dd(strategy_curve),\n",
    "                max_dd(gross_curve),\n",
    "                max_dd(equal_weight_curve),\n",
    "                max_dd(top3_curve),\n",
    "            ],\n",
    "            \"avg_turnover\": [\n",
    "                bt_df[\"turnover\"].mean(),\n",
    "                bt_df[\"turnover\"].mean(),\n",
    "                0.0,\n",
    "                np.nan,\n",
    "            ],\n",
    "        },\n",
    "        index=[\"strategy_net\", \"strategy_gross\", \"equal_weight\", \"top3_predicted\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"predictions\": pred_df,\n",
    "        \"actuals\": actual_df,\n",
    "        \"backtest_df\": bt_df,\n",
    "        \"ic_by_date\": ic_by_date,\n",
    "        \"equity_curve\": pd.Series(strategy_curve.values, index=bt_df[\"date\"]),\n",
    "        \"gross_curve\": pd.Series(gross_curve.values, index=bt_df[\"date\"]),\n",
    "        \"equal_weight_curve\": pd.Series(equal_weight_curve.values, index=bt_df[\"date\"]),\n",
    "        \"top3_curve\": pd.Series(top3_curve.values, index=bt_df[\"date\"]),\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "bt = backtest_model_fast(\n",
    "    df,\n",
    ")\n",
    "\n",
    "display(bt[\"metrics\"].round(4))\n",
    "\n",
    "ax = bt[\"equity_curve\"].plot(figsize=(10, 4), title=\"Fast Out-of-Sample Backtest\", label=\"strategy_net\")\n",
    "bt[\"gross_curve\"].plot(ax=ax, label=\"strategy_gross\")\n",
    "bt[\"equal_weight_curve\"].plot(ax=ax, label=\"equal_weight\")\n",
    "bt[\"top3_curve\"].plot(ax=ax, label=\"top3_predicted\")\n",
    "ax.legend()\n",
    "\n",
    "display(bt[\"backtest_df\"][[\"date\", \"selected_count\", \"gross_return\", \"net_return\", \"turnover\"]].tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
